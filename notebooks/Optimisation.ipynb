{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Klasterinė analizė: Pagrindinės sąvokos \n",
    "# *(Cluster analysis: Basic concepts)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Klasterinė analizė priskiriama prie *(\"Unsupervised learning\")* Mokymasis be mokytojo t.y. nėra iš anksto nustatytų klasių."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "                                import os\n",
    "                                import pandas                         as pd\n",
    "                                import matplotlib.pyplot              as plt\n",
    "                                import numpy                          as np\n",
    "# from sklearn                    import cluster\n",
    "# from sklearn                    import mixture\n",
    "# from collections                import defaultdict\n",
    "# from sklearn.metrics.cluster    import normalized_mutual_info_score\n",
    "# from sklearn.metrics.cluster    import adjusted_rand_score\n",
    "\n",
    "from sklearn.cluster            import DBSCAN\n",
    "from sklearn.cluster            import Birch\n",
    "from sklearn                    import metrics\n",
    "                                import hdbscan\n",
    "                                import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comment this if the data visualisations doesn't work\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "plt.rcParams['grid.linestyle'] = ':'\n",
    "plt.rcParams['grid.color'] = '#474545'\n",
    "plt.rcParams['axes.edgecolor'] = '#474545'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "File path: /workspaces/optimisation_shortest_path/notebooks\nFile name: Clusterization.ipynb\n"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "__file__ = 'Clusterization.ipynb'\n",
    "__path__ = os.path.dirname(os.path.realpath(__file__))\n",
    "\n",
    "print('File path: %s' % __path__)\n",
    "print('File name: %s' %__file__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "(265, 357)"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../output/mungy/data_nm.csv').set_index('ID')\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Methods for clustering:\n",
    "    \n",
    "* [HDBSCAN](http://hdbscan.readthedocs.io/en/latest/index.html)\n",
    "* DBSCAN\n",
    "* BIRCH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Worcking with dictionarys](https://stackoverflow.com/a/8381589/7347438)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Artumo matas *(Proximity measure)*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise   import euclidean_distances\n",
    "from sklearn.metrics.pairwise   import cosine_distances\n",
    "from sklearn.metrics.pairwise   import manhattan_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Expected clusters qty: 8\nLenght of dataframe: 265\n"
    }
   ],
   "source": [
    "kClusters = 8\n",
    "idx = list(df.index.values)\n",
    "results = {'Product': idx}\n",
    "\n",
    "euclidean = euclidean_distances(df, df)\n",
    "manhattan = manhattan_distances(df, df)\n",
    "cosine = cosine_distances(df, df)\n",
    "\n",
    "print('Expected clusters qty: %s' % kClusters)\n",
    "print('Lenght of dataframe: %s' % len(idx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Euclidean distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distance_matrix = pd.DataFrame(euclidean_distances(df, df), \n",
    "#                          index = labels, \n",
    "#                          columns = labeals)\n",
    "# distance_matrix.to_csv('output/{}.csv'.format(euclidean_distances.__name__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Klasterizavimo metodai *(Clusterisation algorithms)*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_grid(parameter_grid):\n",
    "    \"\"\"Input: {x:[y]}, Output: [{x:y}]\"\"\"\n",
    "    return [list_of_toople_to_dic([*zip(parameter_grid.keys(), values)]) \n",
    "            for values \n",
    "            in [*itertools.product(*parameter_grid.values())]]        \n",
    "        \n",
    "def list_of_toople_to_dic(values):\n",
    "    \"\"\"Input: [(x,y)], Output: {x:y}\"\"\"\n",
    "    return {key:value for key, value in values}\n",
    "\n",
    "def add_metrics(model, values, dic):\n",
    "    labels = model.labels_\n",
    "    n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "    n_noise_ = list(labels).count(-1)\n",
    "\n",
    "    dic['Estimated number of clusters'].append(n_clusters_)\n",
    "    dic['Estimated number of noise points'].append(n_noise_)\n",
    "    dic['Silhouette Coefficient'].append(metrics.silhouette_score(df.values, labels)\\\n",
    "        if n_clusters_ != 0 else -1)\n",
    "    dic['Minimum cluster size'].append(min([count_matching(label, labels) for label in np.unique(labels)]))\n",
    "\n",
    "    return dic\n",
    "\n",
    "def count_matching(condition, seq):\n",
    "    \"\"\"Returns the amount of items in seq that return true from condition\"\"\"\n",
    "    return sum(1 for item in seq if condition == item)\n",
    "\n",
    "clust_results = {\n",
    "        'Clasterisation method' : [],\n",
    "        'Maximum distance between two samples = eps' : [],\n",
    "        'The number of samples = min_sample' : [],\n",
    "        'Metric for Distance between instances = metric' :[],\n",
    "        'Minimum cluster size' : [],\n",
    "        'Estimated number of clusters' : [],\n",
    "        'Estimated number of noise points' : [],\n",
    "        'Silhouette Coefficient' : []\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute DBSCAN\n",
    "\n",
    "parameter_grid = dict(\n",
    "    eps=np.round(np.arange(0.1, 7, 0.1), decimals=1),\n",
    "    min_samples = np.round(np.arange(1, 10, 1), decimals=0),\n",
    "    metric = ['euclidean'], #, 'manhattan'\n",
    ")\n",
    "\n",
    "options_grid = split_grid(parameter_grid)\n",
    "\n",
    "for options in options_grid:\n",
    "\n",
    "    model = DBSCAN(n_jobs=-1, **options).fit(df.values)\n",
    "    labels = model.labels_\n",
    "\n",
    "    clust_results['Clasterisation method'].append('DBSCAN')\n",
    "    clust_results['Maximum distance between two samples = eps'].append(options['eps'])\n",
    "    clust_results['The number of samples = min_sample'].append(options['min_samples'])\n",
    "    clust_results['Metric for Distance between instances = metric'].append(options['metric'])\n",
    "\n",
    "    clust_results = add_metrics(model, df.values, clust_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HDBSCAN\n",
    "\n",
    "HDBSCAN - theese method contane several steps:\n",
    "\n",
    "    1. Transform the space according to the density/sparsity.\n",
    "    2. Build the minimum spanning tree of the distance weighted graph.\n",
    "    3. Construct a cluster hierarchy of connected components.\n",
    "    4. Condense the cluster hierarchy based on minimum cluster size.\n",
    "    5. Extract the stable clusters from the condensed tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "parameter_grid = dict(\n",
    "    min_cluster_size = getattr(np.round(np.arange(5, 20, 1, dtype=int), decimals=0), \"tolist\", lambda: value)(),\n",
    "    metric = ['euclidean'], #, 'manhattan'\n",
    ")\n",
    "\n",
    "options_grid = split_grid(parameter_grid)\n",
    "\n",
    "for options in options_grid:\n",
    "\n",
    "    model = hdbscan.HDBSCAN(gen_min_span_tree=True, **options).fit(df.values)\n",
    "\n",
    "    clust_results['Clasterisation method'].append('HDBSCAN')\n",
    "    clust_results['Maximum distance between two samples = eps'].append(None)\n",
    "    clust_results['The number of samples = min_sample'].append(None)\n",
    "    clust_results['Metric for Distance between instances = metric'].append(options['metric'])\n",
    "\n",
    "    clust_results = add_metrics(model, df.values, clust_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BIRCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter_grid = dict(\n",
    "    n_clusters = getattr(np.round(np.arange(5, 40, 1, dtype=int), decimals=0), \"tolist\", lambda: value)(),\n",
    ")\n",
    "\n",
    "options_grid = split_grid(parameter_grid)\n",
    "\n",
    "for options in options_grid:\n",
    "\n",
    "    model = Birch(**options).fit(df.values)\n",
    "\n",
    "    clust_results['Clasterisation method'].append('BIRCH')\n",
    "    clust_results['Maximum distance between two samples = eps'].append(None)\n",
    "    clust_results['The number of samples = min_sample'].append(None)\n",
    "    clust_results['Metric for Distance between instances = metric'].append(None)\n",
    "    \n",
    "    clust_results = add_metrics(model, df.values, clust_results) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Clasterisation method</th>\n      <th>Maximum distance between two samples = eps</th>\n      <th>The number of samples = min_sample</th>\n      <th>Metric for Distance between instances = metric</th>\n      <th>Minimum cluster size</th>\n      <th>Estimated number of clusters</th>\n      <th>Estimated number of noise points</th>\n      <th>Silhouette Coefficient</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>DBSCAN</td>\n      <td>0.1</td>\n      <td>1.0</td>\n      <td>euclidean</td>\n      <td>1</td>\n      <td>90</td>\n      <td>0</td>\n      <td>0.890560</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>DBSCAN</td>\n      <td>0.1</td>\n      <td>2.0</td>\n      <td>euclidean</td>\n      <td>2</td>\n      <td>69</td>\n      <td>21</td>\n      <td>0.822395</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>DBSCAN</td>\n      <td>0.1</td>\n      <td>3.0</td>\n      <td>euclidean</td>\n      <td>3</td>\n      <td>39</td>\n      <td>81</td>\n      <td>0.476191</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>DBSCAN</td>\n      <td>0.1</td>\n      <td>4.0</td>\n      <td>euclidean</td>\n      <td>4</td>\n      <td>33</td>\n      <td>99</td>\n      <td>0.401993</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>DBSCAN</td>\n      <td>0.1</td>\n      <td>5.0</td>\n      <td>euclidean</td>\n      <td>5</td>\n      <td>12</td>\n      <td>183</td>\n      <td>0.106212</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "  Clasterisation method  Maximum distance between two samples = eps  \\\n0                DBSCAN                                         0.1   \n1                DBSCAN                                         0.1   \n2                DBSCAN                                         0.1   \n3                DBSCAN                                         0.1   \n4                DBSCAN                                         0.1   \n\n   The number of samples = min_sample  \\\n0                                 1.0   \n1                                 2.0   \n2                                 3.0   \n3                                 4.0   \n4                                 5.0   \n\n  Metric for Distance between instances = metric  Minimum cluster size  \\\n0                                      euclidean                     1   \n1                                      euclidean                     2   \n2                                      euclidean                     3   \n3                                      euclidean                     4   \n4                                      euclidean                     5   \n\n   Estimated number of clusters  Estimated number of noise points  \\\n0                            90                                 0   \n1                            69                                21   \n2                            39                                81   \n3                            33                                99   \n4                            12                               183   \n\n   Silhouette Coefficient  \n0                0.890560  \n1                0.822395  \n2                0.476191  \n3                0.401993  \n4                0.106212  "
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clust_results = pd.DataFrame(clust_results)\n",
    "clust_results.to_csv('../output/metrics/{}.csv'.format('clust_results'))\n",
    "clust_results.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Silhouette Coefficient is calculated using the mean intra-cluster distance ( a ) and the mean nearest-cluster distance ( b ) for each sample. ... To obtain the values for each sample, use silhouette_samples . The best value is 1 and the worst value is -1. Values near 0 indicate overlapping clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tikrinimas / vertinimas rezultatų *(Validation of the results)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rezultatų interpretavimas *(Interpretation of the results)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics & Plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Otput:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = hdbscan.HDBSCAN(gen_min_span_tree=True, min_cluster_size=6).fit(df.values)\n",
    "labels = model.labels_\n",
    "results = pd.DataFrame(labels, columns=['labels'], index = df.index.values)\n",
    "results.to_csv('../output/models/labels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import subprocess\n",
    "\n",
    "# command = f'jupyter nbconvert Exploratory_analysis.ipynb --output Exploratory_analysis.html'\n",
    "# subprocess.call(command)\n",
    "\n",
    "from IPython.display import Javascript\n",
    "from nbconvert import HTMLExporter\n",
    "from IPython.display import Javascript\n",
    "\n",
    "def save_notebook():\n",
    "    display(\n",
    "        Javascript(\"IPython.notebook.save_notebook()\"),\n",
    "        include=['application/javascript']\n",
    "    )\n",
    "\n",
    "def output_HTML(read_file, output_file):\n",
    "    import codecs\n",
    "    import nbformat\n",
    "    exporter = HTMLExporter()\n",
    "    # read_file is '.ipynb', output_file is '.html'\n",
    "    output_notebook = nbformat.read(read_file, as_version=4)\n",
    "    output, resources = exporter.from_notebook_node(output_notebook)\n",
    "    codecs.open(output_file, 'w', encoding='utf-8').write(output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}